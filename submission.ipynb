{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YNNQoI50k1Fq",
        "Z5jwzzTSmErY",
        "UucuXIjtmc7N",
        "hZdawmQ_moe1",
        "exG_jMcGod00",
        "MqQyQKBip9Qu",
        "IDcaOyAxtPOd",
        "_Nfeg6hNnLLN",
        "CMwKgY2QreCQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# VCL\n",
        "\n",
        "The code is taken from here, and changed to make it adaptable with python 3 and tensorflow 2 (the original code was written for python 2 and tensorflow 1)\n",
        "https://github.com/nvcuong/variational-continual-learning/blob/master/ddm/alg/cla_models_multihead.py"
      ],
      "metadata": {
        "id": "YNNQoI50k1Fq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lmIQB-dktGH"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "np.random.seed(0)\n",
        "tf.set_random_seed(0)\n",
        "\n",
        "# variable initialization functions\n",
        "def weight_variable(shape, init_weights=None):\n",
        "    if init_weights is not None:\n",
        "        initial = tf.constant(init_weights)\n",
        "    else:\n",
        "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def small_variable(shape):\n",
        "    initial = tf.constant(-6.0, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def zero_variable(shape):\n",
        "    initial = tf.zeros(shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def _create_weights_mf(in_dim, hidden_size, out_dim, init_weights=None, init_variances=None):\n",
        "    size = deepcopy(hidden_size)\n",
        "    size.append(out_dim)\n",
        "    size.insert(0, in_dim)\n",
        "    no_params = 0\n",
        "    for i in range(len(size) - 1):\n",
        "        no_weights = size[i] * size[i+1]\n",
        "        no_biases = size[i+1]\n",
        "        no_params += (no_weights + no_biases)\n",
        "    m_weights = weight_variable([no_params], init_weights)\n",
        "    if init_variances is None:\n",
        "        v_weights = small_variable([no_params])\n",
        "    else:\n",
        "        v_weights = tf.Variable(tf.constant(init_variances, dtype=tf.float32))\n",
        "    return no_params, m_weights, v_weights, size\n",
        "\n",
        "class Cla_NN(object):\n",
        "    def __init__(self, input_size, hidden_size, output_size, training_size):\n",
        "        # input and output placeholders\n",
        "        self.x = tf.placeholder(tf.float32, [None, input_size])\n",
        "        self.y = tf.placeholder(tf.float32, [None, output_size])\n",
        "        self.task_idx = tf.placeholder(tf.int32)\n",
        "\n",
        "    def assign_optimizer(self, learning_rate=0.001):\n",
        "        self.train_step = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
        "\n",
        "    def assign_session(self):\n",
        "        # Initializing the variables\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        # launch a session\n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(init)\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, x_train, y_train, task_idx, no_epochs=1000, batch_size=100, display_epoch=5):\n",
        "        N = x_train.shape[0]\n",
        "        if batch_size > N:\n",
        "            batch_size = N\n",
        "\n",
        "        costs = []\n",
        "        # Training cycle\n",
        "        for epoch in range(no_epochs):\n",
        "            perm_inds = list(range(x_train.shape[0]))\n",
        "            np.random.shuffle(perm_inds)\n",
        "            cur_x_train = x_train[perm_inds]\n",
        "            cur_y_train = y_train[perm_inds]\n",
        "\n",
        "            avg_cost = 0.\n",
        "            total_batch = int(np.ceil(N * 1.0 / batch_size))\n",
        "            # Loop over all batches\n",
        "            for i in range(total_batch):\n",
        "                start_ind = i*batch_size\n",
        "                end_ind = np.min([(i+1)*batch_size, N])\n",
        "                batch_x = cur_x_train[start_ind:end_ind, :]\n",
        "                batch_y = cur_y_train[start_ind:end_ind, :]\n",
        "                # Run optimization op (backprop) and cost op (to get loss value)\n",
        "                _, c = self.sess.run(\n",
        "                    [self.train_step, self.cost],\n",
        "                    feed_dict={self.x: batch_x, self.y: batch_y, self.task_idx: task_idx})\n",
        "                # Compute average loss\n",
        "                avg_cost += c / total_batch\n",
        "            # Display logs per epoch step\n",
        "            if epoch % display_epoch == 0:\n",
        "                print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
        "                    \"{:.9f}\".format(avg_cost))\n",
        "            costs.append(avg_cost)\n",
        "        print(\"Optimization Finished!\")\n",
        "        return costs\n",
        "\n",
        "    def prediction(self, x_test, task_idx):\n",
        "        # Test model\n",
        "        prediction = self.sess.run([self.pred], feed_dict={self.x: x_test, self.task_idx: task_idx})[0]\n",
        "        return prediction\n",
        "\n",
        "    def prediction_prob(self, x_test, task_idx):\n",
        "        prob = self.sess.run([tf.nn.softmax(self.pred)], feed_dict={self.x: x_test, self.task_idx: task_idx})[0]\n",
        "        return prob\n",
        "\n",
        "    def get_weights(self):\n",
        "        weights = self.sess.run([self.weights])[0]\n",
        "        return weights\n",
        "\n",
        "    def close_session(self):\n",
        "        self.sess.close()\n",
        "\n",
        "\n",
        "\"\"\" Neural Network Model \"\"\"\n",
        "class Vanilla_NN(Cla_NN):\n",
        "    def __init__(self, input_size, hidden_size, output_size, training_size, prev_weights=None, learning_rate=0.001):\n",
        "\n",
        "        super(Vanilla_NN, self).__init__(input_size, hidden_size, output_size, training_size)\n",
        "        # init weights and biases\n",
        "        self.W, self.b, self.W_last, self.b_last, self.size = self.create_weights(\n",
        "                input_size, hidden_size, output_size, prev_weights)\n",
        "        self.no_layers = len(hidden_size) + 1\n",
        "        self.pred = self._prediction(self.x, self.task_idx)\n",
        "        self.cost = - self._logpred(self.x, self.y, self.task_idx)\n",
        "        self.weights = [self.W, self.b, self.W_last, self.b_last]\n",
        "\n",
        "        self.assign_optimizer(learning_rate)\n",
        "        self.assign_session()\n",
        "\n",
        "    def _prediction(self, inputs, task_idx):\n",
        "        act = inputs\n",
        "        for i in range(self.no_layers-1):\n",
        "            pre = tf.add(tf.matmul(act, self.W[i]), self.b[i])\n",
        "            act = tf.nn.relu(pre)\n",
        "        pre = tf.add(tf.matmul(act, tf.gather(self.W_last, task_idx)), tf.gather(self.b_last, task_idx))\n",
        "        return pre\n",
        "\n",
        "    def _logpred(self, inputs, targets, task_idx):\n",
        "        pred = self._prediction(inputs, task_idx)\n",
        "        log_lik = - tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=targets))\n",
        "        return log_lik\n",
        "\n",
        "    def create_weights(self, in_dim, hidden_size, out_dim, prev_weights):\n",
        "        hidden_size = deepcopy(hidden_size)\n",
        "        hidden_size.append(out_dim)\n",
        "        hidden_size.insert(0, in_dim)\n",
        "        no_params = 0\n",
        "        no_layers = len(hidden_size) - 1\n",
        "        W = []\n",
        "        b = []\n",
        "        W_last = []\n",
        "        b_last = []\n",
        "        for i in range(no_layers-1):\n",
        "            din = hidden_size[i]\n",
        "            dout = hidden_size[i+1]\n",
        "            if prev_weights is None:\n",
        "                Wi_val = tf.truncated_normal([din, dout], stddev=0.1)\n",
        "                bi_val = tf.truncated_normal([dout], stddev=0.1)\n",
        "            else:\n",
        "                Wi_val = tf.constant(prev_weights[0][i])\n",
        "                bi_val = tf.constant(prev_weights[1][i])\n",
        "            Wi = tf.Variable(Wi_val)\n",
        "            bi = tf.Variable(bi_val)\n",
        "            W.append(Wi)\n",
        "            b.append(bi)\n",
        "\n",
        "        if prev_weights is not None:\n",
        "            prev_Wlast = prev_weights[2]\n",
        "            prev_blast = prev_weights[3]\n",
        "            no_prev_tasks = len(prev_Wlast)\n",
        "            for j in range(no_prev_tasks):\n",
        "                W_j = prev_Wlast[j]\n",
        "                b_j = prev_blast[j]\n",
        "                Wi = tf.Variable(W_j)\n",
        "                bi = tf.Variable(b_j)\n",
        "                W_last.append(Wi)\n",
        "                b_last.append(bi)\n",
        "\n",
        "        din = hidden_size[-2]\n",
        "        dout = hidden_size[-1]\n",
        "        Wi_val = tf.truncated_normal([din, dout], stddev=0.1)\n",
        "        bi_val = tf.truncated_normal([dout], stddev=0.1)\n",
        "        Wi = tf.Variable(Wi_val)\n",
        "        bi = tf.Variable(bi_val)\n",
        "        W_last.append(Wi)\n",
        "        b_last.append(bi)\n",
        "\n",
        "        return W, b, W_last, b_last, hidden_size\n",
        "\n",
        "\n",
        "\"\"\" Bayesian Neural Network with Mean field VI approximation \"\"\"\n",
        "class MFVI_NN(Cla_NN):\n",
        "    def __init__(self, input_size, hidden_size, output_size, training_size,\n",
        "        no_train_samples=10, no_pred_samples=100, prev_means=None, prev_log_variances=None, learning_rate=0.001,\n",
        "        prior_mean=0, prior_var=1):\n",
        "        self.in_size = input_size\n",
        "        self.out_size = output_size\n",
        "        super(MFVI_NN, self).__init__(input_size, hidden_size, output_size, training_size)\n",
        "        m, v, self.size = self.create_weights(\n",
        "            input_size, hidden_size, output_size, prev_means, prev_log_variances)\n",
        "        self.W_m, self.b_m, self.W_last_m, self.b_last_m = m[0], m[1], m[2], m[3]\n",
        "        self.W_v, self.b_v, self.W_last_v, self.b_last_v = v[0], v[1], v[2], v[3]\n",
        "        self.weights = [m, v]\n",
        "\n",
        "        m, v = self.create_prior(input_size, hidden_size, output_size, prev_means, prev_log_variances, prior_mean, prior_var)\n",
        "        self.prior_W_m, self.prior_b_m, self.prior_W_last_m, self.prior_b_last_m = m[0], m[1], m[2], m[3]\n",
        "        self.prior_W_v, self.prior_b_v, self.prior_W_last_v, self.prior_b_last_v = v[0], v[1], v[2], v[3]\n",
        "\n",
        "        self.no_layers = len(self.size) - 1\n",
        "        self.no_train_samples = no_train_samples\n",
        "        self.no_pred_samples = no_pred_samples\n",
        "        self.pred = self._prediction(self.x, self.task_idx, self.no_pred_samples)\n",
        "        self.cost = tf.div(self._KL_term(), training_size) - self._logpred(self.x, self.y, self.task_idx)\n",
        "        #self.cost = tf.div(self._KL_term(), training_size)\n",
        "        self.assign_optimizer(learning_rate)\n",
        "        self.assign_session()\n",
        "\n",
        "    def _prediction(self, inputs, task_idx, no_samples):\n",
        "        return self._prediction_layer(inputs, task_idx, no_samples)\n",
        "\n",
        "    # this samples a layer at a time\n",
        "    def _prediction_layer(self, inputs, task_idx, no_samples):\n",
        "        K = no_samples\n",
        "        act = tf.tile(tf.expand_dims(inputs, 0), [K, 1, 1])\n",
        "        for i in range(self.no_layers-1):\n",
        "            din = self.size[i]\n",
        "            dout = self.size[i+1]\n",
        "            eps_w = tf.random_normal((K, din, dout), 0, 1, dtype=tf.float32)\n",
        "            eps_b = tf.random_normal((K, 1, dout), 0, 1, dtype=tf.float32)\n",
        "\n",
        "            weights = tf.add(tf.multiply(eps_w, tf.exp(0.5*self.W_v[i])), self.W_m[i])\n",
        "            biases = tf.add(tf.multiply(eps_b, tf.exp(0.5*self.b_v[i])), self.b_m[i])\n",
        "\n",
        "            act = tf.cast(act,tf.float32)\n",
        "            pre = tf.add(tf.einsum('mni,mio->mno', act, weights), biases)\n",
        "            act = tf.nn.relu(pre)\n",
        "        din = self.size[-2]\n",
        "        dout = self.size[-1]\n",
        "        eps_w = tf.random_normal((K, din, dout), 0, 1, dtype=tf.float32)\n",
        "        eps_b = tf.random_normal((K, 1, dout), 0, 1, dtype=tf.float32)\n",
        "\n",
        "        Wtask_m = tf.gather(self.W_last_m, task_idx)\n",
        "        Wtask_v = tf.gather(self.W_last_v, task_idx)\n",
        "        btask_m = tf.gather(self.b_last_m, task_idx)\n",
        "        btask_v = tf.gather(self.b_last_v, task_idx)\n",
        "        weights = tf.add(tf.multiply(eps_w, tf.exp(0.5*Wtask_v)), Wtask_m)\n",
        "        biases = tf.add(tf.multiply(eps_b, tf.exp(0.5*btask_v)), btask_m)\n",
        "        act = tf.expand_dims(act, 3)\n",
        "        weights = tf.expand_dims(weights, 1)\n",
        "        pre = tf.add(tf.reduce_sum(act * weights, 2), biases)\n",
        "\n",
        "        return pre\n",
        "\n",
        "    def compute_gradients(self, x_batch, y_batch, task_idx):\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            gradients = tf.gradients(self._KL_term() - self._logpred(x_batch.reshape(1,self.in_size)\n",
        "                                                                           , y_batch.reshape(1,self.out_size), 0), tf.trainable_variables())\n",
        "            gradients = [grad for grad in gradients if grad is not None]\n",
        "            gradients = sess.run(gradients)\n",
        "\n",
        "        return gradients\n",
        "\n",
        "    def _logpred(self, inputs, targets, task_idx):\n",
        "        pred = self._prediction(inputs, task_idx, self.no_train_samples)\n",
        "        targets = tf.tile(tf.expand_dims(targets, 0), [self.no_train_samples, 1, 1])\n",
        "        log_lik = - tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=targets))\n",
        "        return log_lik\n",
        "\n",
        "    def _KL_term(self):\n",
        "        kl = 0\n",
        "        for i in range(self.no_layers-1):\n",
        "            din = self.size[i]\n",
        "            dout = self.size[i+1]\n",
        "            m, v = self.W_m[i], self.W_v[i]\n",
        "            m0, v0 = self.prior_W_m[i], self.prior_W_v[i]\n",
        "            const_term = -0.5 * dout * din\n",
        "            log_std_diff = 0.5 * tf.reduce_sum(np.log(v0) - v)\n",
        "            mu_diff_term = 0.5 * tf.reduce_sum((tf.exp(v) + (m0 - m)**2) / v0)\n",
        "            kl += const_term + log_std_diff + mu_diff_term\n",
        "\n",
        "            m, v = self.b_m[i], self.b_v[i]\n",
        "            m0, v0 = self.prior_b_m[i], self.prior_b_v[i]\n",
        "            const_term = -0.5 * dout\n",
        "            log_std_diff = 0.5 * tf.reduce_sum(np.log(v0) - v)\n",
        "            mu_diff_term = 0.5 * tf.reduce_sum((tf.exp(v) + (m0 - m)**2) / v0)\n",
        "            kl += const_term + log_std_diff + mu_diff_term\n",
        "\n",
        "        no_tasks = len(self.W_last_m)\n",
        "        din = self.size[-2]\n",
        "        dout = self.size[-1]\n",
        "        for i in range(no_tasks):\n",
        "            m, v = self.W_last_m[i], self.W_last_v[i]\n",
        "            m0, v0 = self.prior_W_last_m[i], self.prior_W_last_v[i]\n",
        "            const_term = -0.5 * dout * din\n",
        "            log_std_diff = 0.5 * tf.reduce_sum(np.log(v0) - v)\n",
        "            mu_diff_term = 0.5 * tf.reduce_sum((tf.exp(v) + (m0 - m)**2) / v0)\n",
        "            kl += const_term + log_std_diff + mu_diff_term\n",
        "\n",
        "            m, v = self.b_last_m[i], self.b_last_v[i]\n",
        "            m0, v0 = self.prior_b_last_m[i], self.prior_b_last_v[i]\n",
        "            const_term = -0.5 * dout\n",
        "            log_std_diff = 0.5 * tf.reduce_sum(np.log(v0) - v)\n",
        "            mu_diff_term = 0.5 * tf.reduce_sum((tf.exp(v) + (m0 - m)**2) / v0)\n",
        "            kl += const_term + log_std_diff + mu_diff_term\n",
        "        return kl\n",
        "\n",
        "    def create_weights(self, in_dim, hidden_size, out_dim, prev_weights, prev_variances):\n",
        "        hidden_size = deepcopy(hidden_size)\n",
        "        hidden_size.append(out_dim)\n",
        "        hidden_size.insert(0, in_dim)\n",
        "        no_params = 0\n",
        "        no_layers = len(hidden_size) - 1\n",
        "        W_m = []\n",
        "        b_m = []\n",
        "        W_last_m = []\n",
        "        b_last_m = []\n",
        "        W_v = []\n",
        "        b_v = []\n",
        "        W_last_v = []\n",
        "        b_last_v = []\n",
        "        for i in range(no_layers-1):\n",
        "            din = hidden_size[i]\n",
        "            dout = hidden_size[i+1]\n",
        "            if prev_weights is None:\n",
        "                Wi_m_val = tf.truncated_normal([din, dout], stddev=0.1)\n",
        "                bi_m_val = tf.truncated_normal([dout], stddev=0.1)\n",
        "                Wi_v_val = tf.constant(-6.0, shape=[din, dout])\n",
        "                bi_v_val = tf.constant(-6.0, shape=[dout])\n",
        "            else:\n",
        "                Wi_m_val = prev_weights[0][i]\n",
        "                bi_m_val = prev_weights[1][i]\n",
        "                if prev_variances is None:\n",
        "                    Wi_v_val = tf.constant(-6.0, shape=[din, dout])\n",
        "                    bi_v_val = tf.constant(-6.0, shape=[dout])\n",
        "                else:\n",
        "                    Wi_v_val = prev_variances[0][i]\n",
        "                    bi_v_val = prev_variances[1][i]\n",
        "\n",
        "            Wi_m = tf.Variable(Wi_m_val)\n",
        "            bi_m = tf.Variable(bi_m_val)\n",
        "            Wi_v = tf.Variable(Wi_v_val)\n",
        "            bi_v = tf.Variable(bi_v_val)\n",
        "            W_m.append(Wi_m)\n",
        "            b_m.append(bi_m)\n",
        "            W_v.append(Wi_v)\n",
        "            b_v.append(bi_v)\n",
        "\n",
        "        # if there are previous tasks\n",
        "        if prev_weights is not None and prev_variances is not None:\n",
        "            prev_Wlast_m = prev_weights[2]\n",
        "            prev_blast_m = prev_weights[3]\n",
        "            prev_Wlast_v = prev_variances[2]\n",
        "            prev_blast_v = prev_variances[3]\n",
        "            no_prev_tasks = len(prev_Wlast_m)\n",
        "            for i in range(no_prev_tasks):\n",
        "                W_i_m = prev_Wlast_m[i]\n",
        "                b_i_m = prev_blast_m[i]\n",
        "                Wi_m = tf.Variable(W_i_m)\n",
        "                bi_m = tf.Variable(b_i_m)\n",
        "\n",
        "                W_i_v = prev_Wlast_v[i]\n",
        "                b_i_v = prev_blast_v[i]\n",
        "                Wi_v = tf.Variable(W_i_v)\n",
        "                bi_v = tf.Variable(b_i_v)\n",
        "\n",
        "                W_last_m.append(Wi_m)\n",
        "                b_last_m.append(bi_m)\n",
        "                W_last_v.append(Wi_v)\n",
        "                b_last_v.append(bi_v)\n",
        "\n",
        "        din = hidden_size[-2]\n",
        "        dout = hidden_size[-1]\n",
        "\n",
        "        # if point estimate is supplied\n",
        "        if prev_weights is not None and prev_variances is None:\n",
        "            Wi_m_val = prev_weights[2][0]\n",
        "            bi_m_val = prev_weights[3][0]\n",
        "        else:\n",
        "            Wi_m_val = tf.truncated_normal([din, dout], stddev=0.1)\n",
        "            bi_m_val = tf.truncated_normal([dout], stddev=0.1)\n",
        "        Wi_v_val = tf.constant(-6.0, shape=[din, dout])\n",
        "        bi_v_val = tf.constant(-6.0, shape=[dout])\n",
        "\n",
        "        Wi_m = tf.Variable(Wi_m_val)\n",
        "        bi_m = tf.Variable(bi_m_val)\n",
        "        Wi_v = tf.Variable(Wi_v_val)\n",
        "        bi_v = tf.Variable(bi_v_val)\n",
        "        W_last_m.append(Wi_m)\n",
        "        b_last_m.append(bi_m)\n",
        "        W_last_v.append(Wi_v)\n",
        "        b_last_v.append(bi_v)\n",
        "\n",
        "        return [W_m, b_m, W_last_m, b_last_m], [W_v, b_v, W_last_v, b_last_v], hidden_size\n",
        "\n",
        "    def create_prior(self, in_dim, hidden_size, out_dim, prev_weights, prev_variances, prior_mean, prior_var):\n",
        "        hidden_size = deepcopy(hidden_size)\n",
        "        hidden_size.append(out_dim)\n",
        "        hidden_size.insert(0, in_dim)\n",
        "        no_params = 0\n",
        "        no_layers = len(hidden_size) - 1\n",
        "        W_m = []\n",
        "        b_m = []\n",
        "        W_last_m = []\n",
        "        b_last_m = []\n",
        "        W_v = []\n",
        "        b_v = []\n",
        "        W_last_v = []\n",
        "        b_last_v = []\n",
        "        for i in range(no_layers-1):\n",
        "            din = hidden_size[i]\n",
        "            dout = hidden_size[i+1]\n",
        "            if prev_weights is not None and prev_variances is not None:\n",
        "                Wi_m = prev_weights[0][i]\n",
        "                bi_m = prev_weights[1][i]\n",
        "                Wi_v = np.exp(prev_variances[0][i])\n",
        "                bi_v = np.exp(prev_variances[1][i])\n",
        "            else:\n",
        "                Wi_m = prior_mean\n",
        "                bi_m = prior_mean\n",
        "                Wi_v = prior_var\n",
        "                bi_v = prior_var\n",
        "\n",
        "            W_m.append(Wi_m)\n",
        "            b_m.append(bi_m)\n",
        "            W_v.append(Wi_v)\n",
        "            b_v.append(bi_v)\n",
        "\n",
        "        # if there are previous tasks\n",
        "        if prev_weights is not None and prev_variances is not None:\n",
        "            prev_Wlast_m = prev_weights[2]\n",
        "            prev_blast_m = prev_weights[3]\n",
        "            prev_Wlast_v = prev_variances[2]\n",
        "            prev_blast_v = prev_variances[3]\n",
        "            no_prev_tasks = len(prev_Wlast_m)\n",
        "            for i in range(no_prev_tasks):\n",
        "                Wi_m = prev_Wlast_m[i]\n",
        "                bi_m = prev_blast_m[i]\n",
        "                Wi_v = np.exp(prev_Wlast_v[i])\n",
        "                bi_v = np.exp(prev_blast_v[i])\n",
        "\n",
        "                W_last_m.append(Wi_m)\n",
        "                b_last_m.append(bi_m)\n",
        "                W_last_v.append(Wi_v)\n",
        "                b_last_v.append(bi_v)\n",
        "\n",
        "        din = hidden_size[-2]\n",
        "        dout = hidden_size[-1]\n",
        "        Wi_m = prior_mean\n",
        "        bi_m = prior_mean\n",
        "        Wi_v = prior_var\n",
        "        bi_v = prior_var\n",
        "        W_last_m.append(Wi_m)\n",
        "        b_last_m.append(bi_m)\n",
        "        W_last_v.append(Wi_v)\n",
        "        b_last_v.append(bi_v)\n",
        "\n",
        "        return [W_m, b_m, W_last_m, b_last_m], [W_v, b_v, W_last_v, b_last_v]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions\n",
        "\n",
        "Same code as https://github.com/nvcuong/variational-continual-learning/blob/master/ddm/alg/utils.py, adapted for python 3\n",
        "\n",
        "We also changed the plot function to adapt it for our project"
      ],
      "metadata": {
        "id": "Z5jwzzTSmErY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_coresets(x_coresets, y_coresets):\n",
        "    merged_x, merged_y = x_coresets[0], y_coresets[0]\n",
        "    for i in range(1, len(x_coresets)):\n",
        "        merged_x = np.vstack((merged_x, x_coresets[i]))\n",
        "        merged_y = np.vstack((merged_y, y_coresets[i]))\n",
        "    return merged_x, merged_y\n",
        "\n",
        "def get_scores(model, x_testsets, y_testsets, x_coresets, y_coresets, hidden_size, no_epochs, single_head, batch_size=None):\n",
        "    mf_weights, mf_variances = model.get_weights()\n",
        "    acc = []\n",
        "\n",
        "    if single_head:\n",
        "        if len(x_coresets) > 0:\n",
        "            x_train, y_train = merge_coresets(x_coresets, y_coresets)\n",
        "            bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
        "            final_model = MFVI_NN(x_train.shape[1], hidden_size, y_train.shape[1], x_train.shape[0], prev_means=mf_weights, prev_log_variances=mf_variances)\n",
        "            final_model.train(x_train, y_train, 0, no_epochs, bsize)\n",
        "        else:\n",
        "            final_model = model\n",
        "\n",
        "    for i in range(len(x_testsets)):\n",
        "        if not single_head:\n",
        "            if len(x_coresets) > 0:\n",
        "                x_train, y_train = x_coresets[i], y_coresets[i]\n",
        "                bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
        "                final_model = MFVI_NN(x_train.shape[1], hidden_size, y_train.shape[1], x_train.shape[0], prev_means=mf_weights, prev_log_variances=mf_variances)\n",
        "                final_model.train(x_train, y_train, i, no_epochs, bsize)\n",
        "            else:\n",
        "                final_model = model\n",
        "\n",
        "        head = 0 if single_head else i\n",
        "        x_test, y_test = x_testsets[i], y_testsets[i]\n",
        "\n",
        "        pred = final_model.prediction_prob(x_test, head)\n",
        "        pred_mean = np.mean(pred, axis=0)\n",
        "        pred_y = np.argmax(pred_mean, axis=1)\n",
        "        y = np.argmax(y_test, axis=1)\n",
        "        cur_acc = len(np.where((pred_y - y) == 0)[0]) * 1.0 / y.shape[0]\n",
        "        acc.append(cur_acc)\n",
        "\n",
        "        if len(x_coresets) > 0 and not single_head:\n",
        "            final_model.close_session()\n",
        "\n",
        "    if len(x_coresets) > 0 and single_head:\n",
        "        final_model.close_session()\n",
        "\n",
        "    return acc\n",
        "\n",
        "def concatenate_results(score, all_score):\n",
        "    if all_score.size == 0:\n",
        "        all_score = np.reshape(score, (1,-1))\n",
        "    else:\n",
        "        new_arr = np.empty((all_score.shape[0], all_score.shape[1]+1))\n",
        "        new_arr[:] = np.nan\n",
        "        new_arr[:,:-1] = all_score\n",
        "        all_score = np.vstack((new_arr, score))\n",
        "    return all_score\n",
        "\n",
        "def plot(data):\n",
        "    fig, axs = plt.subplots(1, 5, figsize=(16, 4))\n",
        "\n",
        "    for i, ax in enumerate(axs.flat):\n",
        "        ax.plot(range(1,6), data[0][:, i], marker='o',label = 'No coreset')\n",
        "        ax.plot(range(1,6), data[1][:, i], marker='o',label = 'Random')\n",
        "        ax.plot(range(1,6), data[2][:, i], marker='o',label = 'K-center')\n",
        "        ax.plot(range(1,6), data[3][:, i], marker='o',label = 'Forgetting')\n",
        "        ax.plot(range(1,6), data[4][:, i], marker='o',label = 'Herding')\n",
        "        ax.plot(range(1,6), data[5][:, i], marker='o',label = 'Entropy')\n",
        "        ax.set_title(f'Task {i+1}')\n",
        "        ax.set_xlabel('Task')\n",
        "        ax.set_ylabel('Accuracy')\n",
        "        ax.set_ylim(0.7,1.0)\n",
        "        ax.set_xlim(0.5, 5.5)  # Setting x-axis range from 0.5 to 10.5\n",
        "        ax.set_xticks(np.arange(1, 6, 1))  # Setting x-axis ticks from 1 to 10\n",
        "        ax.set_xticklabels(range(1, 6))  # Setting x-axis tick labels from 1 to 10\n",
        "\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    fig.legend(handles, labels, bbox_to_anchor=(0.5, 1.1),loc='upper center',fontsize='large',ncol=len(labels))\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"coreset3\",bbox_inches='tight')"
      ],
      "metadata": {
        "id": "3p83cG41mEGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run VCL\n",
        "\n",
        "Taken from https://github.com/nvcuong/variational-continual-learning/blob/master/ddm/alg/vcl.py, adapted for python 3"
      ],
      "metadata": {
        "id": "UucuXIjtmc7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def run_vcl(hidden_size, no_epochs, data_gen, coreset_method, coreset_size=0, batch_size=None, single_head=True):\n",
        "    in_dim, out_dim = data_gen.get_dims()\n",
        "    x_coresets, y_coresets = [], []\n",
        "    x_testsets, y_testsets = [], []\n",
        "\n",
        "    all_acc = np.array([])\n",
        "\n",
        "    for task_id in range(data_gen.max_iter):\n",
        "        x_train, y_train, x_test, y_test = data_gen.next_task()\n",
        "        x_testsets.append(x_test)\n",
        "        y_testsets.append(y_test)\n",
        "        # Set the readout head to train\n",
        "        head = 0 if single_head else task_id\n",
        "        bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
        "\n",
        "        # Train network with maximum likelihood to initialize first model\n",
        "        if task_id == 0:\n",
        "            ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
        "            ml_model.train(x_train, y_train, task_id, no_epochs, bsize)\n",
        "            mf_weights = ml_model.get_weights()\n",
        "            mf_variances = None\n",
        "            ml_model.close_session()\n",
        "\n",
        "        # Select coreset if needed\n",
        "        if coreset_size > 0:\n",
        "            x_coresets, y_coresets, x_train, y_train = coreset_method(x_coresets, y_coresets, x_train, y_train, coreset_size,in_dim,hidden_size,out_dim,mf_weights,mf_variances,head)\n",
        "\n",
        "        # Train on non-coreset data\n",
        "        mf_model = MFVI_NN(in_dim, hidden_size, out_dim, x_train.shape[0], prev_means=mf_weights, prev_log_variances=mf_variances)\n",
        "        mf_model.train(x_train, y_train, head, no_epochs, bsize)\n",
        "        mf_weights, mf_variances = mf_model.get_weights()\n",
        "\n",
        "        # Incorporate coreset data and make prediction\n",
        "        acc = get_scores(mf_model, x_testsets, y_testsets, x_coresets, y_coresets, hidden_size, no_epochs, single_head, batch_size)\n",
        "        all_acc = concatenate_results(acc, all_acc)\n",
        "\n",
        "        mf_model.close_session()\n",
        "\n",
        "    return all_acc"
      ],
      "metadata": {
        "id": "T84ktV4Xmhh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coreset selection\n",
        "\n",
        "entropy_selection, forgetting_selection, sampling_selection and herding_center was written by me.\n",
        "\n",
        "rand_from_batch and k_center is from https://github.com/nvcuong/variational-continual-learning/blob/master/ddm/alg/coreset.py,"
      ],
      "metadata": {
        "id": "hZdawmQ_moe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\"\"\" Random coreset selection \"\"\"\n",
        "def rand_from_batch(x_coreset, y_coreset, x_train, y_train, coreset_size,in_dim,hidden_size,out_dim,mf_weights,mf_variances,head):\n",
        "    # Randomly select from (x_train, y_train) and add to current coreset (x_coreset, y_coreset)\n",
        "    idx = np.random.choice(x_train.shape[0], coreset_size, False)\n",
        "    x_coreset.append(x_train[idx,:])\n",
        "    y_coreset.append(y_train[idx,:])\n",
        "    x_train = np.delete(x_train, idx, axis=0)\n",
        "    y_train = np.delete(y_train, idx, axis=0)\n",
        "    return x_coreset, y_coreset, x_train, y_train\n",
        "\n",
        "\"\"\" K-center coreset selection \"\"\"\n",
        "def k_center(x_coreset, y_coreset, x_train, y_train, coreset_size,in_dim,hidden_size,out_dim,mf_weights,mf_variances,head):\n",
        "    # Select K centers from (x_train, y_train) and add to current coreset (x_coreset, y_coreset)\n",
        "    dists = np.full(x_train.shape[0], np.inf)\n",
        "    current_id = 0\n",
        "    dists = update_distance(dists, x_train, current_id)\n",
        "    idx = [ current_id ]\n",
        "\n",
        "    for i in range(1, coreset_size):\n",
        "        current_id = np.argmax(dists)\n",
        "        dists = update_distance(dists, x_train, current_id)\n",
        "        idx.append(current_id)\n",
        "\n",
        "    x_coreset.append(x_train[idx,:])\n",
        "    y_coreset.append(y_train[idx,:])\n",
        "    x_train = np.delete(x_train, idx, axis=0)\n",
        "    y_train = np.delete(y_train, idx, axis=0)\n",
        "\n",
        "    return x_coreset, y_coreset, x_train, y_train\n",
        "\n",
        "def entropy_selection(x_coreset, y_coreset, x_train, y_train, coreset_size,in_dim,hidden_size,out_dim,mf_weights,mf_variances,head):\n",
        "    perm_indices = np.random.permutation(len(x_train))\n",
        "    x_train = x_train[perm_indices]\n",
        "    y_train = y_train[perm_indices]\n",
        "    mf_model = MFVI_NN(in_dim, hidden_size, out_dim, x_train.shape[0], prev_means=mf_weights, prev_log_variances=mf_variances)\n",
        "    batch_size = 10000\n",
        "    N = x_train.shape[0]\n",
        "    T = np.full(x_train.shape[0],0)\n",
        "    total_batch = int(np.ceil(N * 1.0 / batch_size))\n",
        "    for i in range(total_batch):\n",
        "        print(i)\n",
        "        start_ind = i*batch_size\n",
        "        end_ind = np.min([(i+1)*batch_size, N])\n",
        "        batch_x = x_train[start_ind:end_ind, :]\n",
        "        pred = mf_model.prediction_prob(batch_x, head)\n",
        "\n",
        "        pred_mean = np.mean(pred, axis=0)\n",
        "\n",
        "\n",
        "        pred_mean_log = np.log(pred_mean)\n",
        "\n",
        "        entropy = -pred_mean * pred_mean_log\n",
        "\n",
        "        sums = np.sum(entropy,axis = 1)\n",
        "\n",
        "        T[start_ind:end_ind] = sums\n",
        "\n",
        "    inds = np.argpartition(T, -coreset_size)[-coreset_size:].astype(int)\n",
        "\n",
        "    mask = np.full(len(x_train), True)  # Create a mask with all True\n",
        "    mask[inds] = False  # Set selected indices to False\n",
        "    x_coreset.append(x_train[~mask])\n",
        "    y_coreset.append(y_train[~mask])\n",
        "    x_train = x_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    return x_coreset, y_coreset, x_train, y_train\n",
        "\n",
        "def forgetting_selection(x_coreset, y_coreset, x_train, y_train, coreset_size,in_dim,hidden_size,out_dim,mf_weights,mf_variances,head):\n",
        "    perm_indices = np.random.permutation(len(x_train))\n",
        "    x_train = x_train[perm_indices]\n",
        "    y_train = y_train[perm_indices]\n",
        "    mf_model = MFVI_NN(in_dim, hidden_size, out_dim, x_train.shape[0], prev_means=mf_weights, prev_log_variances=mf_variances)\n",
        "\n",
        "    T = np.full(x_train.shape[0],0)\n",
        "    prev_acc = np.full(x_train.shape[0],0)\n",
        "    classification = np.full(x_train.shape[0],False)\n",
        "    # use a large batch size\n",
        "    batch_size = 10000\n",
        "\n",
        "    N = x_train.shape[0]\n",
        "    # Loop over all batches\n",
        "    for e in range(20):\n",
        "        total_batch = int(np.ceil(N * 1.0 / batch_size))\n",
        "        for i in range(total_batch):\n",
        "            print(i)\n",
        "            start_ind = i*batch_size\n",
        "            end_ind = np.min([(i+1)*batch_size, N])\n",
        "            batch_x = x_train[start_ind:end_ind, :]\n",
        "            batch_y = y_train[start_ind:end_ind, :]\n",
        "\n",
        "            pred = mf_model.prediction_prob(batch_x, head)\n",
        "            pred_mean = np.mean(pred, axis=0)\n",
        "            pred_y = np.argmax(pred_mean, axis=1)\n",
        "            y = np.argmax(batch_y, axis=1)\n",
        "            ind = -1\n",
        "\n",
        "            for j in range(start_ind,end_ind):\n",
        "                ind+=1\n",
        "                if(pred_y[ind] == y[ind]):\n",
        "                    classification[j] = True\n",
        "                if prev_acc[j] > (pred_y[ind] == y[ind]):\n",
        "                    T[j] = T[j] + 1\n",
        "                prev_acc[j] = (pred_y[ind] == y[ind])\n",
        "\n",
        "            mf_model.train(batch_x, batch_y, 0, 1, batch_x.shape[0])\n",
        "    inds = []\n",
        "    for j in range(N):\n",
        "        if(len(inds) >= coreset_size):\n",
        "            break\n",
        "        if(classification[j] == False):\n",
        "            inds.append(j)\n",
        "    rem = coreset_size - len(inds)\n",
        "    if(rem > 0):\n",
        "        otherMxInds = np.argpartition(T, -rem)[-rem:]\n",
        "        inds = np.concatenate((otherMxInds,inds))\n",
        "        inds = inds.astype(int)\n",
        "    print(inds)\n",
        "    mask = np.ones(len(x_train), dtype=bool)  # Create a mask with all True\n",
        "    mask[inds[:coreset_size]] = False  # Set selected indices to False\n",
        "    x_coreset.append(x_train[~mask])\n",
        "    y_coreset.append(y_train[~mask])\n",
        "    x_train = x_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    return x_coreset, y_coreset, x_train, y_train\n",
        "def compute_score(G,g):\n",
        "    h = np.linalg.norm(g)\n",
        "    k = np.linalg.norm(G, axis=1)\n",
        "    c = np.max(np.divide(np.dot(g, np.array(G).T), (h * k)))\n",
        "    return max(c, 0) + 1  # Ensure the score is positive\n",
        "\n",
        "\n",
        "'''\n",
        "The loss based coreset selection method mentioned in the appendix\n",
        "'''\n",
        "def sampling_selection(x_coreset,y_coreset,x_train,y_train,coreset_size,in_dim,hidden_size,out_dim,mf_weights,mf_variances,head):\n",
        "    perm_indices = np.random.permutation(len(x_train))\n",
        "    x_train = x_train[perm_indices]\n",
        "    y_train = y_train[perm_indices]\n",
        "    mf_model = MFVI_NN(in_dim, hidden_size, out_dim, 1, prev_means=mf_weights, prev_log_variances=mf_variances)\n",
        "    grads = []\n",
        "    res = []\n",
        "    G = []\n",
        "    M = []\n",
        "    C = []\n",
        "    for j in range(x_train.shape[0]):\n",
        "\n",
        "        if(j> 3*coreset_size):\n",
        "            break\n",
        "        g_i = mf_model.compute_gradients(x_train[j], y_train[j], -1)\n",
        "        g_i = np.concatenate([0 if x is None else x.flatten() for x in g_i])\n",
        "        if(j>0):\n",
        "            c = compute_score(G,g_i)\n",
        "        else:\n",
        "            c=0\n",
        "        if(j%10 == 0):\n",
        "            print(c)\n",
        "        if len(M) >= coreset_size:\n",
        "            if(math.isnan(c)):\n",
        "                i = np.random.randint(0, len(M))\n",
        "                r = np.random.uniform(0, 1)\n",
        "                if r < 0.3:\n",
        "                    print(\"Replaced\")\n",
        "                    M[i] = j\n",
        "                    C[i] = 0\n",
        "                    G[i] = g_i\n",
        "            elif c < 1:  # Check if cosine similarity is less than 0\n",
        "\n",
        "                i = np.random.choice(len(M), p=C/np.sum(C))\n",
        "                r = np.random.uniform(0, 1)\n",
        "                if r < C[i] / (C[i] + c):\n",
        "                    print(\"Replaced\")\n",
        "                    M[i] = (j)\n",
        "                    C[i] = c\n",
        "                    G[i] = g_i\n",
        "        else:\n",
        "            if(math.isnan(c)):\n",
        "                c = 0\n",
        "            M.append(j)\n",
        "            C.append(c)\n",
        "            G.append(g_i)\n",
        "\n",
        "    mask = np.ones(len(x_train), dtype=bool)  # Create a mask with all True\n",
        "    mask[M[:coreset_size]] = False  # Set selected indices to False\n",
        "\n",
        "    x_coreset.append(x_train[~mask])\n",
        "    y_coreset.append(y_train[~mask])\n",
        "    x_train = x_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    return x_coreset, y_coreset, x_train, y_train\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" Herding coreset selection \"\"\"\n",
        "def herding_center(x_coreset, y_coreset, x_train, y_train, coreset_size,in_dim,hidden_size,out_dim,mf_weights,mf_variances,head):\n",
        "\n",
        "    dists = np.full(x_train.shape[0], np.inf)\n",
        "    current_cent = x_train[0]\n",
        "    dists = update_distance_herding(dists, x_train, x_train[0])\n",
        "    idx = [ 0 ]\n",
        "    msk = np.full(x_train.shape[0],True)\n",
        "    msk[0] = False\n",
        "    for i in range(1, coreset_size):\n",
        "        filtered_array = dists[msk]\n",
        "        argmax_filtered = np.argmax(filtered_array)\n",
        "        current_id = np.where(msk)[0][argmax_filtered]\n",
        "        res = []\n",
        "        for j in range(x_train[current_id].shape[0]):\n",
        "            res.append((i*current_cent[j] + x_train[current_id][j])/(i+1))\n",
        "        current_cent = np.array(res)\n",
        "        dists = update_distance_herding(dists, x_train, current_cent)\n",
        "        msk[current_id] = False\n",
        "        idx.append(current_id)\n",
        "\n",
        "    x_coreset.append(x_train[idx,:])\n",
        "    y_coreset.append(y_train[idx,:])\n",
        "    x_train = np.delete(x_train, idx, axis=0)\n",
        "    y_train = np.delete(y_train, idx, axis=0)\n",
        "\n",
        "    return x_coreset, y_coreset, x_train, y_train\n",
        "def update_distance_herding(dists, x_train, current_id):\n",
        "    for i in range(x_train.shape[0]):\n",
        "        current_dist = np.linalg.norm(x_train[i,:]-current_id)\n",
        "        dists[i] = current_dist\n",
        "    return dists\n",
        "\n",
        "def update_distance(dists, x_train, current_id):\n",
        "    for i in range(x_train.shape[0]):\n",
        "        current_dist = np.linalg.norm(x_train[i,:]-x_train[current_id,:])\n",
        "        dists[i] = np.minimum(current_dist, dists[i])\n",
        "    return dists"
      ],
      "metadata": {
        "id": "YM-CiTQRmsS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rotated MNIST\n",
        "\n",
        "Code written by me"
      ],
      "metadata": {
        "id": "exG_jMcGod00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import pickle\n",
        "import cv2\n",
        "import sys\n",
        "sys.path.extend(['alg/'])\n",
        "from copy import deepcopy\n",
        "\n",
        "class RotatedMnistGenerator():\n",
        "    def __init__(self, max_iter=5):\n",
        "        with open(\"../input/dataset/mnist.pkl\", 'rb') as f:\n",
        "            print(f)\n",
        "            train_set, valid_set, test_set = pickle.load(f,encoding='latin1')\n",
        "\n",
        "\n",
        "        f.close()\n",
        "\n",
        "        self.X_train = np.vstack((train_set[0], valid_set[0]))\n",
        "        self.Y_train = np.hstack((train_set[1], valid_set[1]))\n",
        "        self.X_test = test_set[0]\n",
        "        self.Y_test = test_set[1]\n",
        "\n",
        "        print(self.Y_train.shape,self.X_train.shape)\n",
        "        print(self.Y_test.shape,self.X_test.shape)\n",
        "        self.max_iter = max_iter\n",
        "        self.cur_iter = 0\n",
        "\n",
        "    def get_dims(self):\n",
        "        # Get data input and output dimensions\n",
        "        return self.X_train.shape[1], 10\n",
        "    def rotate_image(self,img, angle):\n",
        "      # Get rotation matrix\n",
        "        rot_mat = cv2.getRotationMatrix2D((img.shape[1] // 2, img.shape[0] // 2), np.degrees(angle), 1.0)\n",
        "        return cv2.warpAffine(img, rot_mat, (img.shape[1], img.shape[0]))\n",
        "\n",
        "    def next_task(self):\n",
        "        if self.cur_iter >= self.max_iter:\n",
        "            raise Exception('Number of tasks exceeded!')\n",
        "        else:\n",
        "            np.random.seed(self.cur_iter)\n",
        "            angle = np.random.rand() * 2 * np.pi\n",
        "\n",
        "            # Retrieve train data\n",
        "            next_x_train = deepcopy(self.X_train)\n",
        "            next_y_train = np.eye(10)[self.Y_train]\n",
        "            next_x_test = deepcopy(self.X_test)\n",
        "            next_y_test = np.eye(10)[self.Y_test]\n",
        "            for i in range(next_x_train.shape[0]):\n",
        "                img = next_x_train[i].reshape(28, 28)\n",
        "                next_x_train[i] = self.rotate_image(img, angle).reshape(1,28*28)\n",
        "\n",
        "                # Retrieve test data\n",
        "\n",
        "            for i in range(next_x_test.shape[0]):\n",
        "                img = next_x_test[i].reshape(28, 28)\n",
        "                next_x_test[i] = self.rotate_image(img, angle).reshape(1,28*28)\n",
        "\n",
        "\n",
        "            self.cur_iter += 1\n",
        "\n",
        "            return next_x_train, next_y_train, next_x_test, next_y_test\n",
        "\n",
        "hidden_size = [100,100]\n",
        "batch_size = 256\n",
        "no_epochs = 100\n",
        "single_head = True\n",
        "\n",
        "\n",
        "num_tasks = 5\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "coreset_size = 0\n",
        "data_gen = RotatedMnistGenerator()\n",
        "vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    rand_from_batch, coreset_size, batch_size, single_head)\n",
        "print(vcl_result)\n",
        "\n",
        "# Run random coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "coreset_size = 200\n",
        "data_gen = RotatedMnistGenerator()\n",
        "rand_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    rand_from_batch, coreset_size, batch_size, single_head)\n",
        "print(rand_vcl_result)\n",
        "\n",
        "# Run k-center coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "data_gen = RotatedMnistGenerator()\n",
        "kcen_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    k_center, coreset_size, batch_size, single_head)\n",
        "print(kcen_vcl_result)\n",
        "\n",
        "# Run forgetting coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "data_gen = RotatedMnistGenerator()\n",
        "forgetting_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    forgetting_selection, coreset_size, batch_size, single_head)\n",
        "print(forgetting_vcl_result)\n",
        "\n",
        "# Run herding coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "data_gen = RotatedMnistGenerator()\n",
        "herding_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    herding_center, coreset_size, batch_size, single_head)\n",
        "print(herding_vcl_result)\n",
        "\n",
        "\n",
        "# Run entropy coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "data_gen = RotatedMnistGenerator()\n",
        "entropy_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    entropy_selection, coreset_size, batch_size, single_head)\n",
        "print(entropy_vcl_result)\n",
        "\n",
        "\n",
        "# Plot average accuracy\n",
        "vcl_avg = np.nanmean(vcl_result, 1)\n",
        "rand_vcl_avg = np.nanmean(rand_vcl_result, 1)\n",
        "kcen_vcl_avg = np.nanmean(kcen_vcl_result, 1)\n",
        "forgetting_vcl_avg = np.nanmean(forgetting_vcl_result, 1)\n",
        "entropy_vcl_avg = np.nanmean(entropy_vcl_result,1)\n",
        "herding_vcl_avg = np.nanmean(herding_vcl_result,1)\n",
        "\n",
        "print(vcl_avg)\n",
        "print(rand_vcl_avg)\n",
        "print(kcen_vcl_avg)\n",
        "print(forgetting_vcl_avg)\n",
        "print(herding_vcl_avg)\n",
        "print(entropy_vcl_avg)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BaJ76xPsodPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# split notMNIST\n",
        "\n",
        "Code written by me"
      ],
      "metadata": {
        "id": "MqQyQKBip9Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tarfile\n",
        "from scipy import ndimage\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "\n",
        "class notMNISTSplit():\n",
        "    def __init__(self):\n",
        "        with tarfile.open(\"../input/notmnist/notMNIST_small.tar.gz\",\"r:gz\") as tar:\n",
        "            tar.extractall('../working/')\n",
        "        tar.close()\n",
        "        lst1 = ['A','B','C','D','E']\n",
        "        lst2 = ['F','G','H','I','J']\n",
        "        self.X_train = []\n",
        "        self.X_test = []\n",
        "        self.Y_train = []\n",
        "        self.Y_test = []\n",
        "        for i in range(5):\n",
        "            X,Y = self.convert_images2(lst1[i],lst2[i])\n",
        "            X = np.array(X)\n",
        "            Y = np.array(Y)\n",
        "            X_split_train, X_split_test, y_split_train, y_split_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "            y_split_train = np.vstack((y_split_train,1-y_split_train))\n",
        "            y_split_test = np.vstack((y_split_test,1-y_split_test))\n",
        "            y_split_train = y_split_train.transpose()\n",
        "            y_split_test = y_split_test.transpose()\n",
        "            self.X_train.append(X_split_train)\n",
        "            self.X_test.append(X_split_test)\n",
        "            self.Y_train.append(y_split_train)\n",
        "            self.Y_test.append(y_split_test)\n",
        "        self.max_iter = 5\n",
        "        self.cur_iter = 0\n",
        "\n",
        "    def convert_images2(self,a,b):\n",
        "        res_x = []\n",
        "        res_y = []\n",
        "        for filename in os.listdir('../working/notMNIST_small'):\n",
        "            # Construct the full path of the file\n",
        "            if(filename != a and filename != b):\n",
        "                continue\n",
        "            file_path = os.path.join('../working/notMNIST_small', filename)\n",
        "\n",
        "            for imgpath in os.listdir(file_path):\n",
        "                new_path = os.path.join(file_path, imgpath)\n",
        "                image_path = new_path\n",
        "                try:\n",
        "                    image_data = imageio.imread(image_path).flatten()  # Load as 2D array\n",
        "\n",
        "                    res_x.append(image_data)\n",
        "                    if(filename == a):\n",
        "                        res_y.append(0)\n",
        "                    else:\n",
        "                        res_y.append(1)\n",
        "                except Exception as e:\n",
        "                    print(e)\n",
        "        return (res_x, res_y)\n",
        "    def get_dims(self):\n",
        "        # Get data input and output dimensions\n",
        "        return self.X_train[0].shape[1], 2\n",
        "\n",
        "    def next_task(self):\n",
        "        if self.cur_iter >= self.max_iter:\n",
        "            raise Exception('Number of tasks exceeded!')\n",
        "        else:\n",
        "            self.cur_iter += 1\n",
        "            return self.X_train[self.cur_iter-1], self.Y_train[self.cur_iter-1],self.X_test[self.cur_iter-1],self.Y_test[self.cur_iter-1]\n",
        "\n",
        "hidden_size = [150, 150, 150, 150]\n",
        "batch_size = None\n",
        "no_epochs = 100\n",
        "single_head = False\n",
        "\n",
        "\n",
        "num_tasks = 5\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "coreset_size = 0\n",
        "data_gen = notMNISTSplit()\n",
        "vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    rand_from_batch, coreset_size, batch_size, single_head)\n",
        "print(vcl_result)\n",
        "\n",
        "# Run random coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "coreset_size = 200\n",
        "data_gen = notMNISTSplit()\n",
        "rand_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    rand_from_batch, coreset_size, batch_size, single_head)\n",
        "print(rand_vcl_result)\n",
        "\n",
        "# Run k-center coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "data_gen = notMNISTSplit()\n",
        "kcen_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    k_center, coreset_size, batch_size, single_head)\n",
        "print(kcen_vcl_result)\n",
        "\n",
        "# Run forgetting coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "data_gen = notMNISTSplit()\n",
        "forgetting_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    forgetting_selection, coreset_size, batch_size, single_head)\n",
        "print(forgetting_vcl_result)\n",
        "\n",
        "# Run herding coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "data_gen = notMNISTSplit()\n",
        "herding_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    herding_center, coreset_size, batch_size, single_head)\n",
        "print(herding_vcl_result)\n",
        "\n",
        "\n",
        "# Run entropy coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "data_gen = notMNISTSplit()\n",
        "entropy_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    entropy_selection, coreset_size, batch_size, single_head)\n",
        "print(entropy_vcl_result)\n",
        "\n",
        "\n",
        "# Plot average accuracy\n",
        "vcl_avg = np.nanmean(vcl_result, 1)\n",
        "rand_vcl_avg = np.nanmean(rand_vcl_result, 1)\n",
        "kcen_vcl_avg = np.nanmean(kcen_vcl_result, 1)\n",
        "forgetting_vcl_avg = np.nanmean(forgetting_vcl_result, 1)\n",
        "entropy_vcl_avg = np.nanmean(entropy_vcl_result,1)\n",
        "herding_vcl_avg = np.nanmean(herding_vcl_result,1)\n",
        "\n",
        "print(vcl_avg)\n",
        "print(rand_vcl_avg)\n",
        "print(kcen_vcl_avg)\n",
        "print(forgetting_vcl_avg)\n",
        "print(herding_vcl_avg)\n",
        "print(entropy_vcl_avg)\n",
        "\n"
      ],
      "metadata": {
        "id": "ecOGQS3lp_Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR-10\n",
        "\n",
        "Code written by me"
      ],
      "metadata": {
        "id": "IDcaOyAxtPOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "class Cifar10Generator():\n",
        "    def __init__(self, max_iter=5):\n",
        "        self.X_train = []\n",
        "        self.Y_train = []\n",
        "        self.sets_0 = [0, 2, 4, 6, 8]\n",
        "        self.sets_1 = [1, 3, 5, 7, 9]\n",
        "        with open(\"/content/drive/MyDrive/cifar-10-python/cifar-10-batches-py/data_batch_1\", 'rb') as f:\n",
        "            print(f)\n",
        "            dict2 = pickle.load(f,encoding='latin1')\n",
        "            X_train_batch = dict2[\"data\"]\n",
        "            Y_train_batch = dict2[\"labels\"]\n",
        "\n",
        "        f.close()\n",
        "\n",
        "        with open(\"/content/drive/MyDrive/cifar-10-python/cifar-10-batches-py/data_batch_2\", 'rb') as f:\n",
        "            print(f)\n",
        "            dict3 = pickle.load(f,encoding='latin1')\n",
        "            X_train_batch = np.vstack((X_train_batch,dict3[\"data\"]))\n",
        "            Y_train_batch = np.concatenate((Y_train_batch,dict3[\"labels\"]))\n",
        "\n",
        "        f.close()\n",
        "\n",
        "        with open(\"/content/drive/MyDrive/cifar-10-python/cifar-10-batches-py/data_batch_3\", 'rb') as f:\n",
        "            print(f)\n",
        "            dict4 = pickle.load(f,encoding='latin1')\n",
        "            X_train_batch = np.vstack((X_train_batch,dict4[\"data\"]))\n",
        "            Y_train_batch = np.concatenate((Y_train_batch,dict4[\"labels\"]))\n",
        "\n",
        "        f.close()\n",
        "\n",
        "        with open(\"/content/drive/MyDrive/cifar-10-python/cifar-10-batches-py/data_batch_4\", 'rb') as f:\n",
        "            print(f)\n",
        "            dict5 = pickle.load(f,encoding='latin1')\n",
        "            X_train_batch = np.vstack((X_train_batch,dict5[\"data\"]))\n",
        "            Y_train_batch = np.concatenate((Y_train_batch,dict5[\"labels\"]))\n",
        "\n",
        "        f.close()\n",
        "\n",
        "        with open(\"/content/drive/MyDrive/cifar-10-python/cifar-10-batches-py/data_batch_5\", 'rb') as f:\n",
        "            print(f)\n",
        "            dict6 = pickle.load(f,encoding='latin1')\n",
        "            X_train_batch = np.vstack((X_train_batch,dict6[\"data\"]))\n",
        "            Y_train_batch = np.concatenate((Y_train_batch,dict6[\"labels\"]))\n",
        "\n",
        "        f.close()\n",
        "\n",
        "        self.X_train = X_train_batch\n",
        "        self.Y_train = Y_train_batch\n",
        "        with open(\"/content/drive/MyDrive/cifar-10-python/cifar-10-batches-py/test_batch\", 'rb') as f:\n",
        "            print(f)\n",
        "            dict7 = pickle.load(f,encoding='latin1')\n",
        "            self.X_test = dict7[\"data\"]\n",
        "            self.Y_test = dict7[\"labels\"]\n",
        "\n",
        "        f.close()\n",
        "\n",
        "\n",
        "        self.max_iter = 5\n",
        "        self.cur_iter = 0\n",
        "\n",
        "    def get_dims(self):\n",
        "        # Get data input and output dimensions\n",
        "        return self.X_train.shape[1], 2\n",
        "\n",
        "    def next_task(self):\n",
        "        if self.cur_iter >= self.max_iter:\n",
        "            raise Exception('Number of tasks exceeded!')\n",
        "        else:\n",
        "            # Retrieve train data\n",
        "            train_0_id = np.where(self.Y_train == self.sets_0[self.cur_iter])[0]\n",
        "            train_1_id = np.where(self.Y_train == self.sets_1[self.cur_iter])[0]\n",
        "            next_x_train = np.vstack((self.X_train[train_0_id], self.X_train[train_1_id]))\n",
        "\n",
        "            next_y_train = np.vstack((np.ones((train_0_id.shape[0], 1)), np.zeros((train_1_id.shape[0], 1))))\n",
        "            next_y_train = np.hstack((next_y_train, 1-next_y_train))\n",
        "\n",
        "            # Retrieve test data\n",
        "            test_0_id = np.where(np.array(self.Y_test) == self.sets_0[self.cur_iter])[0]\n",
        "            test_1_id = np.where(np.array(self.Y_test) == self.sets_1[self.cur_iter])[0]\n",
        "            next_x_test = np.vstack((self.X_test[test_0_id], self.X_test[test_1_id]))\n",
        "            next_y_test = np.vstack((np.ones((test_0_id.shape[0], 1)), np.zeros((test_1_id.shape[0], 1))))\n",
        "            next_y_test = np.hstack((next_y_test, 1-next_y_test))\n",
        "\n",
        "            self.cur_iter += 1\n",
        "\n",
        "            return next_x_train, next_y_train, next_x_test, next_y_test\n",
        "\n",
        "hidden_size = [150,150]\n",
        "batch_size = 256\n",
        "no_epochs = 100\n",
        "single_head = False\n",
        "\n",
        "\n",
        "num_tasks = 5\n",
        "\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "coreset_size = 0\n",
        "data_gen = Cifar10Generator()\n",
        "vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    rand_from_batch, coreset_size, batch_size, single_head)\n",
        "print(vcl_result)\n",
        "\n",
        "# Run random coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "coreset_size = 200\n",
        "data_gen = Cifar10Generator()\n",
        "rand_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    rand_from_batch, coreset_size, batch_size, single_head)\n",
        "print(rand_vcl_result)\n",
        "\n",
        "# Run k-center coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "data_gen = Cifar10Generator()\n",
        "kcen_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    k_center, coreset_size, batch_size, single_head)\n",
        "print(kcen_vcl_result)\n",
        "\n",
        "# Run forgetting coreset VCL\n",
        "\n",
        "coreset_size = 200\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "data_gen = Cifar10Generator()\n",
        "forgetting_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    forgetting_selection, coreset_size, batch_size, single_head)\n",
        "print(forgetting_vcl_result)\n",
        "\n",
        "# Run herding coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "data_gen = Cifar10Generator()\n",
        "herding_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    herding_center, coreset_size, batch_size, single_head)\n",
        "print(herding_vcl_result)\n",
        "\n",
        "# Run entropy coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "data_gen = Cifar10Generator()\n",
        "entropy_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    entropy_selection, coreset_size, batch_size, single_head)\n",
        "print(entropy_vcl_result)\n",
        "\n",
        "# Plot average accuracy\n",
        "vcl_avg = np.nanmean(vcl_result, 1)\n",
        "rand_vcl_avg = np.nanmean(rand_vcl_result, 1)\n",
        "kcen_vcl_avg = np.nanmean(kcen_vcl_result, 1)\n",
        "forgetting_vcl_avg = np.nanmean(forgetting_vcl_result, 1)\n",
        "entropy_vcl_avg = np.nanmean(entropy_vcl_result,1)\n",
        "herding_vcl_avg = np.nanmean(herding_vcl_result,1)\n",
        "\n",
        "print(vcl_avg)\n",
        "print(rand_vcl_avg)\n",
        "print(kcen_vcl_avg)\n",
        "print(forgetting_vcl_avg)\n",
        "print(entropy_vcl_avg)\n",
        "print(herding_vcl_avg)"
      ],
      "metadata": {
        "id": "oKHOr_5LtUJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Permuted MNIST\n",
        "\n",
        "Adapted from https://github.com/nvcuong/variational-continual-learning/blob/master/ddm/run_permuted.py"
      ],
      "metadata": {
        "id": "_Nfeg6hNnLLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import pickle\n",
        "\n",
        "import sys\n",
        "sys.path.extend(['alg/'])\n",
        "from copy import deepcopy\n",
        "\n",
        "class PermutedMnistGenerator():\n",
        "    def __init__(self, max_iter=10):\n",
        "        with open(\"../input/dataset/mnist.pkl\", 'rb') as f:\n",
        "            print(f)\n",
        "            train_set, valid_set, test_set = pickle.load(f,encoding='latin1')\n",
        "\n",
        "\n",
        "        f.close()\n",
        "\n",
        "        self.X_train = np.vstack((train_set[0], valid_set[0]))\n",
        "        self.Y_train = np.hstack((train_set[1], valid_set[1]))\n",
        "        self.X_test = test_set[0]\n",
        "        self.Y_test = test_set[1]\n",
        "\n",
        "        print(self.Y_train.shape,self.X_train.shape)\n",
        "        print(self.Y_test.shape,self.X_test.shape)\n",
        "        self.max_iter = max_iter\n",
        "        self.cur_iter = 0\n",
        "\n",
        "    def get_dims(self):\n",
        "        # Get data input and output dimensions\n",
        "        return self.X_train.shape[1], 10\n",
        "\n",
        "    def next_task(self):\n",
        "        if self.cur_iter >= self.max_iter:\n",
        "            raise Exception('Number of tasks exceeded!')\n",
        "        else:\n",
        "            np.random.seed(self.cur_iter)\n",
        "            perm_inds = list(range(self.X_train.shape[1]))\n",
        "            np.random.shuffle(perm_inds)\n",
        "\n",
        "            # Retrieve train data\n",
        "            next_x_train = deepcopy(self.X_train)\n",
        "            next_x_train = next_x_train[:,perm_inds]\n",
        "            next_y_train = np.eye(10)[self.Y_train]\n",
        "\n",
        "            # Retrieve test data\n",
        "            next_x_test = deepcopy(self.X_test)\n",
        "            next_x_test = next_x_test[:,perm_inds]\n",
        "            next_y_test = np.eye(10)[self.Y_test]\n",
        "\n",
        "            self.cur_iter += 1\n",
        "\n",
        "            return next_x_train, next_y_train, next_x_test, next_y_test\n",
        "\n",
        "hidden_size = [100, 100]\n",
        "batch_size = 256\n",
        "no_epochs = 100\n",
        "single_head = True\n",
        "num_tasks = 10\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "coreset_size = 0\n",
        "data_gen = PermutedMnistGenerator(num_tasks)\n",
        "vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    rand_from_batch, coreset_size, batch_size, single_head)\n",
        "print(vcl_result)\n",
        "\n",
        "# Run random coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "coreset_size = 200\n",
        "data_gen = PermutedMnistGenerator(num_tasks)\n",
        "rand_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    rand_from_batch, coreset_size, batch_size, single_head)\n",
        "print(rand_vcl_result)\n",
        "\n",
        "# Run k-center coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "data_gen = PermutedMnistGenerator(num_tasks)\n",
        "kcen_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    k_center, coreset_size, batch_size, single_head)\n",
        "print(kcen_vcl_result)\n",
        "\n",
        "# Run forgetting coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "data_gen = PermutedMnistGenerator(num_tasks)\n",
        "forgetting_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    forgetting_selection, coreset_size, batch_size, single_head)\n",
        "print(forgetting_vcl_result)\n",
        "\n",
        "# Run herding coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "data_gen = PermutedMnistGenerator(num_tasks)\n",
        "herding_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    herding_center, coreset_size, batch_size, single_head)\n",
        "print(herding_vcl_result)\n",
        "\n",
        "\n",
        "# Run entropy coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "data_gen = PermutedMnistGenerator(num_tasks)\n",
        "entropy_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    entropy_selection, coreset_size, batch_size, single_head)\n",
        "print(entropy_vcl_result)\n",
        "\n",
        "\n",
        "# Plot average accuracy\n",
        "vcl_avg = np.nanmean(vcl_result, 1)\n",
        "rand_vcl_avg = np.nanmean(rand_vcl_result, 1)\n",
        "kcen_vcl_avg = np.nanmean(kcen_vcl_result, 1)\n",
        "forgetting_vcl_avg = np.nanmean(forgetting_vcl_result, 1)\n",
        "entropy_vcl_avg = np.nanmean(entropy_vcl_result,1)\n",
        "herding_vcl_avg = np.nanmean(herding_vcl_result,1)\n",
        "\n",
        "print(vcl_avg)\n",
        "print(rand_vcl_avg)\n",
        "print(kcen_vcl_avg)\n",
        "print(forgetting_vcl_avg)\n",
        "print(herding_vcl_avg)\n",
        "print(entropy_vcl_avg)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lpz3VmDFnKSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# split MNIST\n",
        "\n",
        "Adapted from https://github.com/nvcuong/variational-continual-learning/blob/master/ddm/run_split.py"
      ],
      "metadata": {
        "id": "CMwKgY2QreCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SplitMnistGenerator():\n",
        "    def __init__(self):\n",
        "        with open(\"../input/dataset/mnist.pkl\", 'rb') as f:\n",
        "            train_set, valid_set, test_set = pickle.load(f,encoding='latin1')\n",
        "        f.close()\n",
        "\n",
        "        self.X_train = np.vstack((train_set[0], valid_set[0]))\n",
        "        self.X_test = test_set[0]\n",
        "        self.train_label = np.hstack((train_set[1], valid_set[1]))\n",
        "        self.test_label = test_set[1]\n",
        "\n",
        "        self.sets_0 = [0, 2, 4, 6, 8]\n",
        "        self.sets_1 = [1, 3, 5, 7, 9]\n",
        "        self.max_iter = len(self.sets_0)\n",
        "        self.cur_iter = 0\n",
        "\n",
        "    def get_dims(self):\n",
        "        # Get data input and output dimensions\n",
        "        return self.X_train.shape[1], 2\n",
        "\n",
        "    def next_task(self):\n",
        "        if self.cur_iter >= self.max_iter:\n",
        "            raise Exception('Number of tasks exceeded!')\n",
        "        else:\n",
        "            # Retrieve train data\n",
        "            train_0_id = np.where(self.train_label == self.sets_0[self.cur_iter])[0]\n",
        "            train_1_id = np.where(self.train_label == self.sets_1[self.cur_iter])[0]\n",
        "            next_x_train = np.vstack((self.X_train[train_0_id], self.X_train[train_1_id]))\n",
        "\n",
        "            next_y_train = np.vstack((np.ones((train_0_id.shape[0], 1)), np.zeros((train_1_id.shape[0], 1))))\n",
        "            next_y_train = np.hstack((next_y_train, 1-next_y_train))\n",
        "\n",
        "            # Retrieve test data\n",
        "            test_0_id = np.where(self.test_label == self.sets_0[self.cur_iter])[0]\n",
        "            test_1_id = np.where(self.test_label == self.sets_1[self.cur_iter])[0]\n",
        "            next_x_test = np.vstack((self.X_test[test_0_id], self.X_test[test_1_id]))\n",
        "\n",
        "            next_y_test = np.vstack((np.ones((test_0_id.shape[0], 1)), np.zeros((test_1_id.shape[0], 1))))\n",
        "            next_y_test = np.hstack((next_y_test, 1-next_y_test))\n",
        "\n",
        "            self.cur_iter += 1\n",
        "\n",
        "            return next_x_train, next_y_train, next_x_test, next_y_test\n",
        "\n",
        "hidden_size = [256, 256]\n",
        "batch_size = None\n",
        "no_epochs = 120\n",
        "single_head = False\n",
        "\n",
        "num_tasks = 5\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "coreset_size = 0\n",
        "data_gen = SplitMnistGenerator()\n",
        "vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    rand_from_batch, coreset_size, batch_size, single_head)\n",
        "print(vcl_result)\n",
        "\n",
        "# Run random coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "coreset_size = 200\n",
        "data_gen = SplitMnistGenerator()\n",
        "rand_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    rand_from_batch, coreset_size, batch_size, single_head)\n",
        "print(rand_vcl_result)\n",
        "\n",
        "# Run k-center coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "data_gen = SplitMnistGenerator()\n",
        "kcen_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    k_center, coreset_size, batch_size, single_head)\n",
        "print(kcen_vcl_result)\n",
        "\n",
        "# Run forgetting coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "data_gen = SplitMnistGenerator()\n",
        "forgetting_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    forgetting_selection, coreset_size, batch_size, single_head)\n",
        "print(forgetting_vcl_result)\n",
        "\n",
        "# Run herding coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "data_gen = SplitMnistGenerator()\n",
        "herding_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    herding_center, coreset_size, batch_size, single_head)\n",
        "print(herding_vcl_result)\n",
        "\n",
        "\n",
        "# Run entropy coreset VCL\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(12)\n",
        "np.random.seed(1)\n",
        "\n",
        "data_gen = SplitMnistGenerator()\n",
        "entropy_vcl_result = run_vcl(hidden_size, no_epochs, data_gen,\n",
        "    entropy_selection, coreset_size, batch_size, single_head)\n",
        "print(entropy_vcl_result)\n",
        "\n",
        "\n",
        "# Plot average accuracy\n",
        "vcl_avg = np.nanmean(vcl_result, 1)\n",
        "rand_vcl_avg = np.nanmean(rand_vcl_result, 1)\n",
        "kcen_vcl_avg = np.nanmean(kcen_vcl_result, 1)\n",
        "forgetting_vcl_avg = np.nanmean(forgetting_vcl_result, 1)\n",
        "entropy_vcl_avg = np.nanmean(entropy_vcl_result,1)\n",
        "herding_vcl_avg = np.nanmean(herding_vcl_result,1)\n",
        "\n",
        "print(vcl_avg)\n",
        "print(rand_vcl_avg)\n",
        "print(kcen_vcl_avg)\n",
        "print(forgetting_vcl_avg)\n",
        "print(herding_vcl_avg)\n",
        "print(entropy_vcl_avg)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UlKqWtgvr6bg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
